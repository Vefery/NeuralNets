{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "lr = 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear():\n",
    "    def __init__(self, input_dim: int, output_dim: int):\n",
    "        gen = np.random.default_rng()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.k = np.sqrt(1.0 / input_dim)\n",
    "        self.weights = gen.uniform(-self.k, self.k, size=(output_dim, input_dim)).astype(np.float32)\n",
    "        self.bias = gen.uniform(-self.k, self.k, size=(output_dim)).astype(np.float32)\n",
    "        # Gradient storage\n",
    "        self.d_weights = np.zeros_like(self.weights)\n",
    "        self.d_bias = np.zeros_like(self.bias)\n",
    "        self.input = None\n",
    "        self.m_weights = np.zeros_like(self.weights)\n",
    "        self.v_weights = np.zeros_like(self.weights)\n",
    "        self.m_bias = np.zeros_like(self.bias)\n",
    "        self.v_bias = np.zeros_like(self.bias)\n",
    "        self.t = 0\n",
    "    \n",
    "    def __call__(self, x: np.ndarray) -> np.ndarray:\n",
    "        self.input = x  # Store input for backward pass\n",
    "        y = x @ self.weights.T + self.bias\n",
    "        return y\n",
    "    \n",
    "    def backward(self, grad_output: np.ndarray) -> np.ndarray:\n",
    "        # Compute gradients\n",
    "        self.d_weights = grad_output.T @ self.input\n",
    "        self.d_bias = grad_output.sum(axis=0)\n",
    "        # Propagate gradient backward\n",
    "        grad_input = grad_output @ self.weights\n",
    "        return grad_input\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        self.d_weights.fill(0)\n",
    "        self.d_bias.fill(0)\n",
    "    \n",
    "def ReLU(x: np.ndarray) -> np.ndarray:\n",
    "    return np.clip(x, min=0)\n",
    "\n",
    "def ReLU_derivative(x: np.ndarray) -> np.ndarray:\n",
    "    return (x > 0).astype(np.float32)\n",
    "\n",
    "class CrossEntropyLoss():\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        self.target = None\n",
    "    \n",
    "    def __call__(self, x: np.ndarray, y: np.ndarray) -> float:\n",
    "        self.input = x\n",
    "        self.target = y\n",
    "        log_softmax = self.Softmax(x)\n",
    "        loss = -log_softmax[np.arange(log_softmax.shape[0]), y]\n",
    "        return loss.mean()\n",
    "    \n",
    "    def Softmax(self, x: np.ndarray) -> np.ndarray:\n",
    "        c = x.max(axis=1).reshape(-1, 1)\n",
    "        lse = c + np.log(np.exp(x - c).sum(axis=1)).reshape(-1, 1)\n",
    "        log_softmax = x - lse\n",
    "        return log_softmax\n",
    "\n",
    "    def backward(self) -> np.ndarray:\n",
    "        batch_size = self.input.shape[0]\n",
    "        # Compute softmax\n",
    "        exp_x = np.exp(self.input - self.input.max(axis=1, keepdims=True))\n",
    "        softmax = exp_x / exp_x.sum(axis=1, keepdims=True)\n",
    "        # Gradient of cross-entropy loss with softmax\n",
    "        grad = softmax.copy()\n",
    "        grad[np.arange(batch_size), self.target] -= 1\n",
    "        grad /= batch_size\n",
    "        return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "t = np.array([0, 2])\n",
    "print(a, \"\\n\")\n",
    "print(t, \"\\n\")\n",
    "print(a[np.arange(a.shape[0]), t])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network():\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int):\n",
    "        self.layer1 = Linear(input_dim, hidden_dim)\n",
    "        self.layer2 = Linear(hidden_dim, hidden_dim)\n",
    "        self.layer3 = Linear(hidden_dim, output_dim)\n",
    "        self.loss_fn = CrossEntropyLoss()\n",
    "        # Storage for intermediate activations\n",
    "        self.z1 = None\n",
    "        self.a1 = None\n",
    "        self.z2 = None\n",
    "        self.a2 = None\n",
    "        self.z3 = None\n",
    "    \n",
    "    def __call__(self, input: np.ndarray) -> np.ndarray:\n",
    "        x = input.reshape(input.shape[0], -1)\n",
    "        self.z1 = self.layer1(x)\n",
    "        self.a1 = ReLU(self.z1)\n",
    "        self.z2 = self.layer2(self.a1)\n",
    "        self.a2 = ReLU(self.z2)\n",
    "        self.z3 = self.layer3(self.a2)\n",
    "        return self.z3\n",
    "    \n",
    "    def backward(self):\n",
    "        # Backward pass through the network\n",
    "        grad = self.loss_fn.backward()\n",
    "        grad = self.layer3.backward(grad)\n",
    "        grad = grad * ReLU_derivative(self.z2)\n",
    "        grad = self.layer2.backward(grad)\n",
    "        grad = grad * ReLU_derivative(self.z1)\n",
    "        grad = self.layer1.backward(grad)\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        self.layer1.zero_grad()\n",
    "        self.layer2.zero_grad()\n",
    "        self.layer3.zero_grad()\n",
    "    \n",
    "    def update(self, lr: float = 0.001, beta1: float = 0.9, beta2: float = 0.999, weight_decay: float = 0.01, eps: float = 1e-8):\n",
    "        for layer in [self.layer1, self.layer2, self.layer3]:\n",
    "            layer.t += 1  # Increment timestep\n",
    "            \n",
    "            # Update weights\n",
    "            layer.m_weights = beta1 * layer.m_weights + (1 - beta1) * layer.d_weights\n",
    "            layer.v_weights = beta2 * layer.v_weights + (1 - beta2) * (layer.d_weights ** 2)\n",
    "            m_hat_w = layer.m_weights / (1 - beta1 ** layer.t)\n",
    "            v_hat_w = layer.v_weights / (1 - beta2 ** layer.t)\n",
    "            layer.weights -= lr * m_hat_w / (np.sqrt(v_hat_w) + eps)  # Adam update\n",
    "            layer.weights -= lr * weight_decay * layer.weights        # Weight decay\n",
    "            \n",
    "            # Update biases\n",
    "            layer.m_bias = beta1 * layer.m_bias + (1 - beta1) * layer.d_bias\n",
    "            layer.v_bias = beta2 * layer.v_bias + (1 - beta2) * (layer.d_bias ** 2)\n",
    "            m_hat_b = layer.m_bias / (1 - beta1 ** layer.t)\n",
    "            v_hat_b = layer.v_bias / (1 - beta2 ** layer.t)\n",
    "            layer.bias -= lr * m_hat_b / (np.sqrt(v_hat_b) + eps)    # Adam update\n",
    "            # Note: Typically no weight decay for biases, but can be added if desired\n",
    "    \n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "        self.linear_relu_stack.apply(init_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "def init_weights(m):\n",
    "    gen = np.random.default_rng(257)\n",
    "    if isinstance(m, nn.Linear):\n",
    "        m.weight.data = torch.tensor(gen.uniform(-1, 1, size=(m.out_features, m.in_features)), dtype=torch.float32)\n",
    "        m.bias.data = torch.tensor(gen.uniform(-1, 1, size=(m.out_features)), dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Linear' object has no attribute 'm_weights'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m     losses\u001b[38;5;241m.\u001b[39mappend(loss)\n\u001b[0;32m     11\u001b[0m     mynet\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 12\u001b[0m     \u001b[43mmynet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m progress\u001b[38;5;241m.\u001b[39mupdate()\n",
      "Cell \u001b[1;32mIn[26], line 42\u001b[0m, in \u001b[0;36mNetwork.update\u001b[1;34m(self, lr, beta1, beta2, weight_decay, eps)\u001b[0m\n\u001b[0;32m     39\u001b[0m layer\u001b[38;5;241m.\u001b[39mt \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# Increment timestep\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Update weights\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m layer\u001b[38;5;241m.\u001b[39mm_weights \u001b[38;5;241m=\u001b[39m beta1 \u001b[38;5;241m*\u001b[39m \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mm_weights\u001b[49m \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1) \u001b[38;5;241m*\u001b[39m layer\u001b[38;5;241m.\u001b[39md_weights\n\u001b[0;32m     43\u001b[0m layer\u001b[38;5;241m.\u001b[39mv_weights \u001b[38;5;241m=\u001b[39m beta2 \u001b[38;5;241m*\u001b[39m layer\u001b[38;5;241m.\u001b[39mv_weights \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2) \u001b[38;5;241m*\u001b[39m (layer\u001b[38;5;241m.\u001b[39md_weights \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     44\u001b[0m m_hat_w \u001b[38;5;241m=\u001b[39m layer\u001b[38;5;241m.\u001b[39mm_weights \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m layer\u001b[38;5;241m.\u001b[39mt)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Linear' object has no attribute 'm_weights'"
     ]
    }
   ],
   "source": [
    "mynet = Network(28*28, 512, 10)\n",
    "\n",
    "losses = []\n",
    "with tqdm(total=25) as progress:\n",
    "    for epoch in range(25):\n",
    "        for feature, lable in train_dataloader:\n",
    "            mynet.zero_grad()\n",
    "            output = mynet(feature.numpy())\n",
    "            loss = mynet.loss_fn(output, lable.numpy())\n",
    "            losses.append(loss)\n",
    "            mynet.backward()\n",
    "            mynet.update(lr=lr)\n",
    "\n",
    "        progress.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "\n",
    "# Add labels and title (optional)\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 64.91%\n"
     ]
    }
   ],
   "source": [
    "all = len(test_data)\n",
    "right = 0\n",
    "for feat, labl in test_data:\n",
    "    out = mynet(feat.numpy())\n",
    "    if np.argmax(np.exp(mynet.loss_fn.Softmax(out))) == labl:\n",
    "        right += 1\n",
    "\n",
    "print(f\"Accuracy: {(right/all) * 100}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NeuralNets",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
